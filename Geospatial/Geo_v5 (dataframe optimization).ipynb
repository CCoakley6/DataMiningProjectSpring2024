{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State fips codes for later merging with the shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_fips_to_full_info = {\n",
    "    '01': ['AL', 'Alabama'],\n",
    "    '02': ['AK', 'Alaska'],\n",
    "    '04': ['AZ', 'Arizona'],\n",
    "    '05': ['AR', 'Arkansas'],\n",
    "    '06': ['CA', 'California'],\n",
    "    '08': ['CO', 'Colorado'],\n",
    "    '09': ['CT', 'Connecticut'],\n",
    "    '10': ['DE', 'Delaware'],\n",
    "    '11': ['DC', 'District of Columbia'],\n",
    "    '12': ['FL', 'Florida'],\n",
    "    '13': ['GA', 'Georgia'],\n",
    "    '15': ['HI', 'Hawaii'],\n",
    "    '16': ['ID', 'Idaho'],\n",
    "    '17': ['IL', 'Illinois'],\n",
    "    '18': ['IN', 'Indiana'],\n",
    "    '19': ['IA', 'Iowa'],\n",
    "    '20': ['KS', 'Kansas'],\n",
    "    '21': ['KY', 'Kentucky'],\n",
    "    '22': ['LA', 'Louisiana'],\n",
    "    '23': ['ME', 'Maine'],\n",
    "    '24': ['MD', 'Maryland'],\n",
    "    '25': ['MA', 'Massachusetts'],\n",
    "    '26': ['MI', 'Michigan'],\n",
    "    '27': ['MN', 'Minnesota'],\n",
    "    '28': ['MS', 'Mississippi'],\n",
    "    '29': ['MO', 'Missouri'],\n",
    "    '30': ['MT', 'Montana'],\n",
    "    '31': ['NE', 'Nebraska'],\n",
    "    '32': ['NV', 'Nevada'],\n",
    "    '33': ['NH', 'New Hampshire'],\n",
    "    '34': ['NJ', 'New Jersey'],\n",
    "    '35': ['NM', 'New Mexico'],\n",
    "    '36': ['NY', 'New York'],\n",
    "    '37': ['NC', 'North Carolina'],\n",
    "    '38': ['ND', 'North Dakota'],\n",
    "    '39': ['OH', 'Ohio'],\n",
    "    '40': ['OK', 'Oklahoma'],\n",
    "    '41': ['OR', 'Oregon'],\n",
    "    '42': ['PA', 'Pennsylvania'],\n",
    "    '44': ['RI', 'Rhode Island'],\n",
    "    '45': ['SC', 'South Carolina'],\n",
    "    '46': ['SD', 'South Dakota'],\n",
    "    '47': ['TN', 'Tennessee'],\n",
    "    '48': ['TX', 'Texas'],\n",
    "    '49': ['UT', 'Utah'],\n",
    "    '50': ['VT', 'Vermont'],\n",
    "    '51': ['VA', 'Virginia'],\n",
    "    '53': ['WA', 'Washington'],\n",
    "    '54': ['WV', 'West Virginia'],\n",
    "    '55': ['WI', 'Wisconsin'],\n",
    "    '56': ['WY', 'Wyoming']\n",
    "}\n",
    "\n",
    "# Invert the dictionary to map state names to FIPS codes\n",
    "name_to_fips = {info[1]: fips for fips, info in state_fips_to_full_info.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loads all csvs in a folder into a df (if they have the necessary columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_csv_to_df(directory):\n",
    "    path = Path(directory)\n",
    "    csv_files = list(path.glob('*.csv'))\n",
    "    all_data_frames = []\n",
    "\n",
    "    required_columns = ['date', 'cumulative_deceased', 'new_deceased', 'subregion2_name', 'population', 'subregion1_name']\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        # Check if required columns are present\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            # Report missing columns and skip this file\n",
    "            print(f\"File '{file_path}' is missing columns: {missing_columns}\")\n",
    "            continue\n",
    "        all_data_frames.append(df)\n",
    "    \n",
    "    # Concatenate all dataframes that have the required columns\n",
    "    combined_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color assignment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# progressive color assignment\n",
    "def assign_color(rate):\n",
    "    \"\"\"Assigns a color based on the normalized incidence rate, handling NaN values.\"\"\"\n",
    "    # Check if rate is NaN\n",
    "    if pd.isna(rate):\n",
    "        # Return a default color, e.g., gray\n",
    "        return '#808080'  # Gray color in hex\n",
    "    else:\n",
    "        red = int(rate * 255)\n",
    "        green = 255 - red\n",
    "        blue = 0  # Blue component remains 0 throughout\n",
    "        # Convert RGB to hex\n",
    "        return '#{:02x}{:02x}{:02x}'.format(red, green, blue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binning color assignment\n",
    "def assign_color_by_bin(rate, num_bins):\n",
    "    if pd.isna(rate) or num_bins <= 0:\n",
    "        # Return a default color, e.g., gray, if the input is NA or num_bins is invalid\n",
    "        return '#808080'  # Gray color in hex\n",
    "    else:\n",
    "        # Calculate the index of the bin to which the rate belongs\n",
    "        bin_index = min(int(rate * num_bins), num_bins - 1)\n",
    "        \n",
    "        # Calculate the color components based on the bin index\n",
    "        red = int(bin_index / (num_bins - 1) * 255)\n",
    "        green = 255 - red\n",
    "        blue = 0  # Blue component remains 0 throughout\n",
    "\n",
    "        # Convert RGB to hex\n",
    "        return '#{:02x}{:02x}{:02x}'.format(red, green, blue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maps state_fips to name of state in original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_state_fips(dataframe, state_name_col='subregion1_name'):\n",
    "    \"\"\"Maps state names to FIPS codes within the provided DataFrame and ensures they are stored as two-digit strings.\"\"\"\n",
    "    dataframe['state_fips'] = dataframe[state_name_col].map(name_to_fips)\n",
    "    dataframe['state_fips'] = dataframe['state_fips'].apply(lambda x: x.zfill(2) if isinstance(x, str) else None)\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(all_data_df, time_frames, color_funcs, save_dir, **kwargs):\n",
    "    # force date column to be datetime\n",
    "    all_data_df['date'] = pd.to_datetime(all_data_df['date'])\n",
    "    \n",
    "    # filter out columns where new_deceased < 0\n",
    "    all_data_df = all_data_df[all_data_df['new_deceased'] >= 0]\n",
    "    if 'subregion1_name' in all_data_df.columns:\n",
    "        all_data_df = map_state_fips(all_data_df)\n",
    "    for time_frame in time_frames:\n",
    "        for color_func in color_funcs:\n",
    "            group_cols = ['state_fips', pd.Grouper(key='date', freq=time_frame)]\n",
    "            agg_dict = {\n",
    "                'population': 'last',\n",
    "                'new_deceased': 'sum',\n",
    "                'cumulative_deceased': 'last'\n",
    "            }\n",
    "            aggregated_data = all_data_df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "            aggregated_data['incidence_rate'] = (aggregated_data['new_deceased'] / aggregated_data['population']) * 100000\n",
    "            min_rate = aggregated_data['incidence_rate'].min()\n",
    "            max_rate = aggregated_data['incidence_rate'].max()\n",
    "            if max_rate > min_rate:\n",
    "                aggregated_data['normalized_incidence_rate'] = (aggregated_data['incidence_rate'] - min_rate) / (max_rate - min_rate)\n",
    "            else:\n",
    "                aggregated_data['normalized_incidence_rate'] = 0.0\n",
    "            if color_func == 'assign_color':\n",
    "                aggregated_data['color'] = aggregated_data['normalized_incidence_rate'].apply(assign_color)\n",
    "            elif color_func == 'assign_color_by_bin':\n",
    "                num_bins = kwargs.get('num_bins', 5)\n",
    "                aggregated_data['color'] = aggregated_data['normalized_incidence_rate'].apply(lambda x: assign_color_by_bin(x, num_bins))\n",
    "            Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "            filename = f\"aggregated_data_{time_frame}_{color_func}.csv\"\n",
    "            aggregated_data.to_csv(Path(save_dir) / filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter the csv to only include the columns_to_keep for later usage in joining with shapefile df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_csv_by_date_and_columns(csv_file_path, date, columns_to_keep):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    filtered_df = df[df['date'] == pd.Timestamp(date)]\n",
    "    filtered_df = filtered_df[columns_to_keep]\n",
    "    filtered_df['state_fips'] = pd.to_numeric(filtered_df['state_fips'], errors='coerce').fillna(-1).astype(int)\n",
    "    filtered_df['state_fips'] = filtered_df['state_fips'].apply(lambda x: f\"{x:02d}\" if x != -1 else None)\n",
    "    filtered_df.dropna(subset=['state_fips'], inplace=True)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with filtered_df with shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "def merge_shapefile_with_data(us_counties, filtered_df, output_file_path=None):\n",
    "    # Ensure the 'state_fips' and 'color' columns are handled correctly\n",
    "    filtered_df['state_fips'] = filtered_df['state_fips'].astype(str)  # Ensure state_fips is string\n",
    "    # Fill missing color values with grey or assign grey if the color column is missing\n",
    "    filtered_df['color'] = filtered_df.get('color', pd.Series(index=filtered_df.index, dtype=str)).fillna('#808080')\n",
    "    \n",
    "    # Merge the filtered DataFrame with the GeoDataFrame on state FIPS and subregion name\n",
    "    merged_gdf = us_counties.merge(filtered_df, left_on=['STATEFP', 'NAMELSAD'], right_on=['state_fips', 'subregion2_name'], how='left')\n",
    "\n",
    "    # Ensure that any rows in merged_gdf without a 'color' value are set to grey\n",
    "    merged_gdf['color'].fillna('#808080', inplace=True)\n",
    "    \n",
    "    # Optionally convert 'date' column to string to ensure compatibility with all file formats\n",
    "    if 'date' in merged_gdf.columns:\n",
    "        merged_gdf['date'] = merged_gdf['date'].astype(str)\n",
    "\n",
    "    # Optionally save the merged GeoDataFrame to a file if an output path is provided\n",
    "    if output_file_path:\n",
    "        merged_gdf.to_file(output_file_path, driver='GeoJSON')  # Using GeoJSON as an example, adjust as necessary\n",
    "\n",
    "    return merged_gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot merged_gdf geospatial plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "def plot_counties_from_merged_gdf(merged_gdf, title, output_png_path):\n",
    "    # Define the continental US bounds\n",
    "    continental_us_bounds = {\n",
    "        \"minx\": -130,\n",
    "        \"miny\": 24,\n",
    "        \"maxx\": -66,\n",
    "        \"maxy\": 50\n",
    "    }\n",
    "    \n",
    "    # Create a figure and axis with specified figsize and resolution (DPI)\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), dpi=300)\n",
    "    \n",
    "    # Set the bounds for the continental US\n",
    "    ax.set_xlim(continental_us_bounds[\"minx\"], continental_us_bounds[\"maxx\"])\n",
    "    ax.set_ylim(continental_us_bounds[\"miny\"], continental_us_bounds[\"maxy\"])\n",
    "    \n",
    "    # Plot all counties in the dataset with a neutral color to provide a base map\n",
    "    merged_gdf.plot(ax=ax, color='lightgrey', edgecolor='black', linewidth=0.4)\n",
    "    \n",
    "    # Overlay the counties with specified colors where available\n",
    "    # Ensure that color values are correctly interpreted\n",
    "    merged_gdf.dropna(subset=['color']).plot(ax=ax, color=merged_gdf['color'], edgecolor='black', linewidth=0.4)\n",
    "    \n",
    "    # Adjust plot parameters\n",
    "    ax.set_title(title)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    # Save the figure to a PNG file with the specified path and high resolution\n",
    "    plt.savefig(output_png_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the functions to generate all plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '..\\All CSVs\\US_AK_02060.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_AK_02105.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_AK_02164.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_AK_02282.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_CA_SFO.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72001.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72003.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72005.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72007.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72009.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72011.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72013.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72015.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72017.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72019.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72021.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72023.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72025.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72027.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72029.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72031.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72033.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72035.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72037.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72039.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72041.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72043.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72045.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72047.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72049.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72051.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72053.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72054.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72055.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72057.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72059.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72061.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72063.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72065.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72067.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72069.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72071.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72073.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72075.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72077.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72079.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72081.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72083.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72085.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72087.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72089.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72091.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72093.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72095.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72097.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72099.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72101.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72103.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72105.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72107.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72109.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72111.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72113.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72115.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72117.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72119.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72121.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72123.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72125.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72127.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72129.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72131.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72133.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72135.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72137.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72139.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72141.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72143.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72145.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72147.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72149.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72151.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n",
      "File '..\\All CSVs\\US_PR_72153.csv' is missing columns: ['cumulative_deceased', 'new_deceased']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Collin Coakley\\AppData\\Local\\Temp\\ipykernel_13064\\2850456368.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  dataframe['state_fips'] = dataframe[state_name_col].map(name_to_fips)\n",
      "C:\\Users\\Collin Coakley\\AppData\\Local\\Temp\\ipykernel_13064\\2850456368.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['state_fips'] = dataframe[state_name_col].map(name_to_fips)\n",
      "C:\\Users\\Collin Coakley\\AppData\\Local\\Temp\\ipykernel_13064\\2850456368.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['state_fips'] = dataframe['state_fips'].apply(lambda x: x.zfill(2) if isinstance(x, str) else None)\n",
      "C:\\Users\\Collin Coakley\\AppData\\Local\\Temp\\ipykernel_13064\\1641386416.py:11: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  group_cols = ['state_fips', pd.Grouper(key='date', freq=time_frame)]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.38 GiB for an array with shape (522, 2412020) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m color_func \u001b[38;5;129;01min\u001b[39;00m color_functions:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m color_func \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massign_color\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 32\u001b[0m         \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_data_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43minterval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolor_func\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m         process_data(all_data_df, [interval], [color_func], output_dir, num_bins\u001b[38;5;241m=\u001b[39mnum_bins)\n",
      "Cell \u001b[1;32mIn[9], line 17\u001b[0m, in \u001b[0;36mprocess_data\u001b[1;34m(all_data_df, time_frames, color_funcs, save_dir, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m group_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_fips\u001b[39m\u001b[38;5;124m'\u001b[39m, pd\u001b[38;5;241m.\u001b[39mGrouper(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, freq\u001b[38;5;241m=\u001b[39mtime_frame)]\n\u001b[0;32m     12\u001b[0m agg_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpopulation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_deceased\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_deceased\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m }\n\u001b[1;32m---> 17\u001b[0m aggregated_data \u001b[38;5;241m=\u001b[39m \u001b[43mall_data_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39magg(agg_dict)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     18\u001b[0m aggregated_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincidence_rate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (aggregated_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_deceased\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m aggregated_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpopulation\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[0;32m     19\u001b[0m min_rate \u001b[38;5;241m=\u001b[39m aggregated_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincidence_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:9156\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9159\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9162\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1054\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         in_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# create the Grouping\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# allow us to passing the actual Grouping as the gpr\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m     ping \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1054\u001b[0m         \u001b[43mGrouping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgroup_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1056\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgpr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1059\u001b[0m \u001b[43m            \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[43m            \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m            \u001b[49m\u001b[43min_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouping)\n\u001b[0;32m   1065\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m gpr\n\u001b[0;32m   1066\u001b[0m     )\n\u001b[0;32m   1068\u001b[0m     groupings\u001b[38;5;241m.\u001b[39mappend(ping)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(groupings) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj):\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\groupby\\grouper.py:579\u001b[0m, in \u001b[0;36mGrouping.__init__\u001b[1;34m(self, index, grouper, obj, level, sort, observed, in_axis, dropna, uniques)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grouping_vector, Grouper):\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# get the new grouper; we already have disambiguated\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     \u001b[38;5;66;03m# what key/level refer to exactly, don't need to\u001b[39;00m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;66;03m# check again as we have by this point converted these\u001b[39;00m\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# to an actual value (rather than a pd.Grouper)\u001b[39;00m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m     newgrouper, newobj \u001b[38;5;241m=\u001b[39m \u001b[43mgrouping_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_grouper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m newobj\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(newgrouper, ops\u001b[38;5;241m.\u001b[39mBinGrouper):\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;66;03m# TODO: can we unwrap this and get a tighter typing\u001b[39;00m\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;66;03m#  for self.grouping_vector?\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\resample.py:2276\u001b[0m, in \u001b[0;36mTimeGrouper._get_grouper\u001b[1;34m(self, obj, validate)\u001b[0m\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_grouper\u001b[39m(\n\u001b[0;32m   2273\u001b[0m     \u001b[38;5;28mself\u001b[39m, obj: NDFrameT, validate: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[BinGrouper, NDFrameT]:\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;66;03m# create the resampler and return our binner\u001b[39;00m\n\u001b[1;32m-> 2276\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_resampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39m_grouper, cast(NDFrameT, r\u001b[38;5;241m.\u001b[39mobj)\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\resample.py:2225\u001b[0m, in \u001b[0;36mTimeGrouper._get_resampler\u001b[1;34m(self, obj, kind)\u001b[0m\n\u001b[0;32m   2223\u001b[0m _, ax, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_grouper(obj, gpr_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   2224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, DatetimeIndex):\n\u001b[1;32m-> 2225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDatetimeIndexResampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimegrouper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgpr_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2232\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, PeriodIndex) \u001b[38;5;129;01mor\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperiod\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ax, PeriodIndex):\n\u001b[0;32m   2235\u001b[0m         \u001b[38;5;66;03m# GH#53481\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\resample.py:185\u001b[0m, in \u001b[0;36mResampler.__init__\u001b[1;34m(self, obj, timegrouper, axis, kind, gpr_index, group_keys, selection, include_groups)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_groups \u001b[38;5;241m=\u001b[39m include_groups\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39max, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timegrouper\u001b[38;5;241m.\u001b[39m_set_grouper(\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, gpr_index\u001b[38;5;241m=\u001b[39mgpr_index\n\u001b[0;32m    186\u001b[0m )\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinner, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grouper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_binner()\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selection \u001b[38;5;241m=\u001b[39m selection\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\resample.py:241\u001b[0m, in \u001b[0;36mResampler._convert_obj\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_obj\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: NDFrameT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m    230\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;124;03m    Provide any conversions for the object in order to correctly handle.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    Series or DataFrame\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6385\u001b[0m, in \u001b[0;36mNDFrame._consolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   6376\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6377\u001b[0m \u001b[38;5;124;03mCompute NDFrame with \"consolidated\" internals (data of each dtype\u001b[39;00m\n\u001b[0;32m   6378\u001b[0m \u001b[38;5;124;03mgrouped together in a single ndarray).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6382\u001b[0m \u001b[38;5;124;03mconsolidated : same type as caller\u001b[39;00m\n\u001b[0;32m   6383\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6384\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mconsolidate()\n\u001b[1;32m-> 6385\u001b[0m cons_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_protect_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(cons_data, axes\u001b[38;5;241m=\u001b[39mcons_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   6387\u001b[0m     \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   6388\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6360\u001b[0m, in \u001b[0;36mNDFrame._protect_consolidate\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   6358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f()\n\u001b[0;32m   6359\u001b[0m blocks_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks)\n\u001b[1;32m-> 6360\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mblocks) \u001b[38;5;241m!=\u001b[39m blocks_before:\n\u001b[0;32m   6362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:6384\u001b[0m, in \u001b[0;36mNDFrame._consolidate.<locals>.<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m   6374\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   6376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6377\u001b[0m \u001b[38;5;124;03m    Compute NDFrame with \"consolidated\" internals (data of each dtype\u001b[39;00m\n\u001b[0;32m   6378\u001b[0m \u001b[38;5;124;03m    grouped together in a single ndarray).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6382\u001b[0m \u001b[38;5;124;03m    consolidated : same type as caller\u001b[39;00m\n\u001b[0;32m   6383\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6384\u001b[0m     f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6385\u001b[0m     cons_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_protect_consolidate(f)\n\u001b[0;32m   6386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(cons_data, axes\u001b[38;5;241m=\u001b[39mcons_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   6387\u001b[0m         \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m   6388\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:621\u001b[0m, in \u001b[0;36mBaseBlockManager.consolidate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes, verify_integrity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    620\u001b[0m bm\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 621\u001b[0m \u001b[43mbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bm\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1787\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1784\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1785\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1787\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1788\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2268\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2266\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2268\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2270\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2271\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\Collin Coakley\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2300\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2297\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2299\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2300\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2301\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2303\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 9.38 GiB for an array with shape (522, 2412020) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set directories and paths\n",
    "csv_files_directory = '../All CSVs'  # Adjust if different\n",
    "directory = 'Modified Data'\n",
    "output_dir = os.path.join(directory, \"Processed CSVs\")\n",
    "shapefile_path = 'tl_2023_us_county.shp'  # Adjust path as needed\n",
    "\n",
    "# Load shapefile\n",
    "shape_df = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Define time intervals and color functions\n",
    "time_intervals = ['2M', '3M']#['3D', '7D', '14D', '21D', '1M', '2M']\n",
    "color_functions = ['assign_color', 'assign_color_by_bin']\n",
    "num_bins = 5  # Number of bins for binning color function, applicable only when using assign_color_by_bin\n",
    "\n",
    "# Ensure output directories exist\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load all CSV files that contain the necessary columns\n",
    "all_data_df = load_all_csv_to_df(csv_files_directory)\n",
    "\n",
    "# Process each time interval for both color functions\n",
    "for interval in time_intervals:\n",
    "    for color_func in color_functions:\n",
    "        if color_func == 'assign_color':\n",
    "            process_data(all_data_df, [interval], [color_func], output_dir)\n",
    "        else:\n",
    "            process_data(all_data_df, [interval], [color_func], output_dir, num_bins=num_bins)\n",
    "\n",
    "# Update file_list to match the new files\n",
    "pattern = os.path.join(output_dir, 'aggregated_data_*.csv')\n",
    "file_list = glob.glob(pattern)\n",
    "\n",
    "# Columns to keep for plotting\n",
    "columns_to_keep = ['date', 'state_fips', 'subregion2_name', 'color']\n",
    "\n",
    "# Iterate through the list of file paths\n",
    "for file_path in file_list:\n",
    "    filename = os.path.basename(file_path)\n",
    "    parts = filename.replace('aggregated_data_', '').replace('.csv', '').split('_')\n",
    "    interval = parts[1]\n",
    "    color_method = parts[0]\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    unique_dates = sorted(df['date'].unique())\n",
    "\n",
    "    for date in unique_dates:\n",
    "        # Filter and merge data for each unique date\n",
    "        filtered_df = filter_csv_by_date_and_columns(file_path, date, columns_to_keep)\n",
    "        merged_gdf = merge_shapefile_with_data(shape_df, filtered_df)\n",
    "\n",
    "        # Plot and save as PNG\n",
    "        plot_title = f\"{date} - {interval} - {color_method}\"\n",
    "        output_png_path = f\"{output_dir}/{date} - {interval} - {color_method}.png\"\n",
    "        plot_counties_from_merged_gdf(merged_gdf, plot_title, output_png_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
