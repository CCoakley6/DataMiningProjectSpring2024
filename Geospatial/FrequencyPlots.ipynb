{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc10a2a-74ed-4a12-b953-31d1dc5bc67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def make_color_lighter(color, rate, num_bins):\n",
    "    \"\"\"\n",
    "    Lighten a color based on the bin that a given rate falls into, and return the result as a hexadecimal string.\n",
    "    Each bin corresponds to a certain percentage of lightening.\n",
    "\n",
    "    Args:\n",
    "    color (tuple): The original color as an RGB tuple with values in the range [0, 1].\n",
    "    rate (float): A value between 0 and 1 that determines which bin the color will be categorized into.\n",
    "    num_bins (int): The number of bins, where each bin represents a different lightening intensity.\n",
    "\n",
    "    Returns:\n",
    "    str: The lightened color as a hexadecimal string.\n",
    "    \"\"\"\n",
    "    if pd.isna(rate):\n",
    "        return '#808080'  # Corrected indentation for returning gray color when rate is NaN\n",
    "\n",
    "    def rgb_to_hex(rgb):\n",
    "        return '#' + ''.join(f\"{int(round(255 * x)):02x}\" for x in rgb)\n",
    "\n",
    "    white = (1, 1, 1)\n",
    "    bin_index = int(rate * num_bins)\n",
    "    percentage = (num_bins - bin_index) / num_bins\n",
    "    lightened_color = tuple(percentage * w + (1 - percentage) * c for c, w in zip(color, white))\n",
    "    return rgb_to_hex(lightened_color)\n",
    "\n",
    "\n",
    "def process_files_and_aggregate_data_generic(csv_files, frequency, save_path):\n",
    "    \"\"\"\n",
    "    Process a list of CSV files to aggregate data according to a specified frequency and save the aggregated data to a specified path.\n",
    "    The function also checks for and reports NaN values within each file, and calculates additional statistics such as incidence rates.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_files: List of file paths to CSV files.\n",
    "    - frequency: String specifying the frequency for data aggregation (e.g., 'M' for monthly, 'Y' for yearly).\n",
    "    - save_path: String specifying the path where the aggregated DataFrame should be saved as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing aggregated data for specified columns across all files. This DataFrame is also saved to the specified path.\n",
    "    \"\"\"\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files provided.\")\n",
    "        return None\n",
    "\n",
    "    aggregated_stats = pd.DataFrame()\n",
    "    total_files = len(csv_files)\n",
    "    files_with_nan = 0\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        total_values = df.size\n",
    "        nan_values = df.isna().sum().sum()\n",
    "        nan_percentage = (nan_values / total_values) * 100\n",
    "        print(f\"File '{file_path}' has {nan_percentage:.2f}% NaN values.\")\n",
    "        if nan_percentage > 0:\n",
    "            files_with_nan += 1\n",
    "\n",
    "        required_columns = ['date', 'cumulative_deceased', 'new_deceased', 'subregion2_name', 'population', 'subregion1_name']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"File '{file_path}' is missing columns: {missing_columns}\")\n",
    "            continue\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        agg_dict = {\n",
    "            'population': 'last',\n",
    "            'new_deceased': 'sum',\n",
    "            'cumulative_deceased': 'last'\n",
    "        }\n",
    "        group_cols = ['subregion1_name', 'subregion2_name'] if 'subregion2_name' in df.columns else ['subregion1_name']\n",
    "        group_cols.append(pd.Grouper(key='date', freq=frequency))\n",
    "        aggregated_data = df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "        # Additional aggregation for vaccination data\n",
    "        if 'new_persons_fully_vaccinated' in df.columns and 'cumulative_persons_fully_vaccinated' in df.columns:\n",
    "            agg_dict.update({\n",
    "                'new_persons_fully_vaccinated': 'sum',\n",
    "                'cumulative_persons_fully_vaccinated': 'last'\n",
    "            })\n",
    "\n",
    "        # Calculate incidence rate and assign colors\n",
    "        if 'new_deceased' in df.columns and 'population' in df.columns:\n",
    "            aggregated_data['incidence_rate'] = (aggregated_data['new_deceased'] / aggregated_data['population']) * 100000\n",
    "            aggregated_data['normalized_incidence_rate'] = (aggregated_data['incidence_rate'] - aggregated_data['incidence_rate'].min()) / (aggregated_data['incidence_rate'].max() - aggregated_data['incidence_rate'].min())\n",
    "            aggregated_data['color'] = aggregated_data['normalized_incidence_rate'].apply(lambda x: make_color_lighter((1, 0, 0), x, 10))\n",
    "\n",
    "        # Append to the main DataFrame\n",
    "        aggregated_stats = pd.concat([aggregated_stats, aggregated_data], ignore_index=True)\n",
    "\n",
    "    # Save aggregated data to specified path\n",
    "    aggregated_stats.to_csv(save_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a74f8a0-726b-493d-b0b8-948eab1ffaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "state_fips_to_full_info = {\n",
    "    '01': ['AL', 'Alabama'],\n",
    "    '02': ['AK', 'Alaska'],\n",
    "    '04': ['AZ', 'Arizona'],\n",
    "    '05': ['AR', 'Arkansas'],\n",
    "    '06': ['CA', 'California'],\n",
    "    '08': ['CO', 'Colorado'],\n",
    "    '09': ['CT', 'Connecticut'],\n",
    "    '10': ['DE', 'Delaware'],\n",
    "    '11': ['DC', 'District of Columbia'],\n",
    "    '12': ['FL', 'Florida'],\n",
    "    '13': ['GA', 'Georgia'],\n",
    "    '15': ['HI', 'Hawaii'],\n",
    "    '16': ['ID', 'Idaho'],\n",
    "    '17': ['IL', 'Illinois'],\n",
    "    '18': ['IN', 'Indiana'],\n",
    "    '19': ['IA', 'Iowa'],\n",
    "    '20': ['KS', 'Kansas'],\n",
    "    '21': ['KY', 'Kentucky'],\n",
    "    '22': ['LA', 'Louisiana'],\n",
    "    '23': ['ME', 'Maine'],\n",
    "    '24': ['MD', 'Maryland'],\n",
    "    '25': ['MA', 'Massachusetts'],\n",
    "    '26': ['MI', 'Michigan'],\n",
    "    '27': ['MN', 'Minnesota'],\n",
    "    '28': ['MS', 'Mississippi'],\n",
    "    '29': ['MO', 'Missouri'],\n",
    "    '30': ['MT', 'Montana'],\n",
    "    '31': ['NE', 'Nebraska'],\n",
    "    '32': ['NV', 'Nevada'],\n",
    "    '33': ['NH', 'New Hampshire'],\n",
    "    '34': ['NJ', 'New Jersey'],\n",
    "    '35': ['NM', 'New Mexico'],\n",
    "    '36': ['NY', 'New York'],\n",
    "    '37': ['NC', 'North Carolina'],\n",
    "    '38': ['ND', 'North Dakota'],\n",
    "    '39': ['OH', 'Ohio'],\n",
    "    '40': ['OK', 'Oklahoma'],\n",
    "    '41': ['OR', 'Oregon'],\n",
    "    '42': ['PA', 'Pennsylvania'],\n",
    "    '44': ['RI', 'Rhode Island'],\n",
    "    '45': ['SC', 'South Carolina'],\n",
    "    '46': ['SD', 'South Dakota'],\n",
    "    '47': ['TN', 'Tennessee'],\n",
    "    '48': ['TX', 'Texas'],\n",
    "    '49': ['UT', 'Utah'],\n",
    "    '50': ['VT', 'Vermont'],\n",
    "    '51': ['VA', 'Virginia'],\n",
    "    '53': ['WA', 'Washington'],\n",
    "    '54': ['WV', 'West Virginia'],\n",
    "    '55': ['WI', 'Wisconsin'],\n",
    "    '56': ['WY', 'Wyoming']\n",
    "}\n",
    "\n",
    "def update_column_names_in_csv(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    csv_files = folder.glob('*.csv')\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if 'subregion2_name' not in df.columns and 'locality_name' in df.columns:\n",
    "            print(f\"Updating '{csv_file.name}': 'locality_name' -> 'subregion2_name'\")\n",
    "            df.rename(columns={'locality_name': 'subregion2_name'}, inplace=True)\n",
    "            df.to_csv(csv_file, index=False)\n",
    "\n",
    "def assign_color(rate):\n",
    "    if pd.isna(rate):\n",
    "        return '#808080'\n",
    "    else:\n",
    "        red = int(rate * 255)\n",
    "        green = 255 - red\n",
    "        blue = 0\n",
    "        return '#{:02x}{:02x}{:02x}'.format(red, green, blue)\n",
    "\n",
    "def process_files_in_folder_pathlib(folder_path):\n",
    "    files_list = []\n",
    "    folder = Path(folder_path)\n",
    "    for file in folder.glob('*.csv'):\n",
    "        if file.is_file():\n",
    "            files_list.append(str(file))\n",
    "        else:\n",
    "            print(f\"{file} is a directory, skipping.\\n\")\n",
    "    return files_list\n",
    "\n",
    "name_to_fips = {info[1]: fips for fips, info in state_fips_to_full_info.items()}\n",
    "\n",
    "\n",
    "def make_color_lighter(color, rate, num_bins):\n",
    "    \"\"\"\n",
    "    Lighten a color based on the bin that a given rate falls into, and return the result as a hexadecimal string.\n",
    "    Each bin corresponds to a certain percentage of lightening.\n",
    "\n",
    "    Args:\n",
    "    color (tuple): The original color as an RGB tuple with values in the range [0, 1].\n",
    "    rate (float): A value between 0 and 1 that determines which bin the color will be categorized into.\n",
    "    num_bins (int): The number of bins, where each bin represents a different lightening intensity.\n",
    "\n",
    "    Returns:\n",
    "    str: The lightened color as a hexadecimal string.\n",
    "    \"\"\"\n",
    "    if pd.isna(rate):\n",
    "        return '#808080'  # Corrected indentation for returning gray color when rate is NaN\n",
    "\n",
    "    def rgb_to_hex(rgb):\n",
    "        return '#' + ''.join(f\"{int(round(255 * x)):02x}\" for x in rgb)\n",
    "\n",
    "    white = (1, 1, 1)\n",
    "    bin_index = int(rate * num_bins)\n",
    "    percentage = (num_bins - bin_index) / num_bins\n",
    "    lightened_color = tuple(percentage * w + (1 - percentage) * c for c, w in zip(color, white))\n",
    "    return rgb_to_hex(lightened_color)\n",
    "\n",
    "def process_files_and_aggregate_data_generic(csv_files, frequency, save_path):\n",
    "    \"\"\"\n",
    "    Process a list of CSV files to aggregate data according to a specified frequency and save the aggregated data to a specified path.\n",
    "    The function also checks for and reports NaN values within each file, and calculates additional statistics such as incidence rates.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_files: List of file paths to CSV files.\n",
    "    - frequency: String specifying the frequency for data aggregation (e.g., 'M' for monthly, 'Y' for yearly).\n",
    "    - save_path: String specifying the path where the aggregated DataFrame should be saved as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing aggregated data for specified columns across all files. This DataFrame is also saved to the specified path.\n",
    "    \"\"\"\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files provided.\")\n",
    "        return None\n",
    "\n",
    "    aggregated_stats = pd.DataFrame()\n",
    "    total_files = len(csv_files)\n",
    "    files_with_nan = 0\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        total_values = df.size\n",
    "        nan_values = df.isna().sum().sum()\n",
    "        nan_percentage = (nan_values / total_values) * 100\n",
    "        print(f\"File '{file_path}' has {nan_percentage:.2f}% NaN values.\")\n",
    "        if nan_percentage > 0:\n",
    "            files_with_nan += 1\n",
    "\n",
    "        required_columns = ['date', 'cumulative_deceased', 'new_deceased', 'subregion2_name', 'population', 'subregion1_name']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            print(f\"File '{file_path}' is missing columns: {missing_columns}\")\n",
    "            continue\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        agg_dict = {\n",
    "            'population': 'last',\n",
    "            'new_deceased': 'sum',\n",
    "            'cumulative_deceased': 'last'\n",
    "        }\n",
    "        group_cols = ['subregion1_name', 'subregion2_name'] if 'subregion2_name' in df.columns else ['subregion1_name']\n",
    "        group_cols.append(pd.Grouper(key='date', freq=frequency))\n",
    "        aggregated_data = df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "        # Additional aggregation for vaccination data\n",
    "        if 'new_persons_fully_vaccinated' in df.columns and 'cumulative_persons_fully_vaccinated' in df.columns:\n",
    "            agg_dict.update({\n",
    "                'new_persons_fully_vaccinated': 'sum',\n",
    "                'cumulative_persons_fully_vaccinated': 'last'\n",
    "            })\n",
    "\n",
    "        # Calculate incidence rate and assign colors\n",
    "        if 'new_deceased' in df.columns and 'population' in df.columns:\n",
    "            aggregated_data['incidence_rate'] = (aggregated_data['new_deceased'] / aggregated_data['population']) * 100000\n",
    "            aggregated_data['normalized_incidence_rate'] = (aggregated_data['incidence_rate'] - aggregated_data['incidence_rate'].min()) / (aggregated_data['incidence_rate'].max() - aggregated_data['incidence_rate'].min())\n",
    "            aggregated_data['color'] = aggregated_data['normalized_incidence_rate'].apply(lambda x: make_color_lighter((1, 0, 0), x, 10))\n",
    "\n",
    "        # Append to the main DataFrame\n",
    "        aggregated_stats = pd.concat([aggregated_stats, aggregated_data], ignore_index=True)\n",
    "\n",
    "    # Save aggregated data to specified path\n",
    "    aggregated_stats.to_csv(save_path, index=False)\n",
    "\n",
    "folder_path = \"../All CSVs\"\n",
    "csv_files = process_files_in_folder_pathlib(folder_path)\n",
    "\n",
    "# Correctly defining frequency list with all values\n",
    "freq_list = ['7D']\n",
    "\n",
    "for freq in freq_list:\n",
    "    output_path = f'Modified Data/aggregated_7D_stats_{freq}.csv'\n",
    "    aggregated_data = process_files_and_aggregate_data_generic(csv_files, freq, output_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f932173f-fc06-4e4f-af76-2b075b3c3d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4ccdb0-e79a-42e1-a844-331dca9fbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac64b4ba-1bac-43b5-8e92-0ff2e9b7865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_color(rate):\n",
    "    if pd.isna(rate):\n",
    "        return '#808080'\n",
    "    else:\n",
    "        red = int(rate * 255)\n",
    "        green = 255 - red\n",
    "        blue = 0\n",
    "        return '#{:02x}{:02x}{:02x}'.format(red, green, blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dadc229e-51e9-4fe5-a2f6-78ffb1ecdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fips_to_full_info = {\n",
    "    '01': ['AL', 'Alabama'],\n",
    "    '02': ['AK', 'Alaska'],\n",
    "    '04': ['AZ', 'Arizona'],\n",
    "    '05': ['AR', 'Arkansas'],\n",
    "    '06': ['CA', 'California'],\n",
    "    '08': ['CO', 'Colorado'],\n",
    "    '09': ['CT', 'Connecticut'],\n",
    "    '10': ['DE', 'Delaware'],\n",
    "    '11': ['DC', 'District of Columbia'],\n",
    "    '12': ['FL', 'Florida'],\n",
    "    '13': ['GA', 'Georgia'],\n",
    "    '15': ['HI', 'Hawaii'],\n",
    "    '16': ['ID', 'Idaho'],\n",
    "    '17': ['IL', 'Illinois'],\n",
    "    '18': ['IN', 'Indiana'],\n",
    "    '19': ['IA', 'Iowa'],\n",
    "    '20': ['KS', 'Kansas'],\n",
    "    '21': ['KY', 'Kentucky'],\n",
    "    '22': ['LA', 'Louisiana'],\n",
    "    '23': ['ME', 'Maine'],\n",
    "    '24': ['MD', 'Maryland'],\n",
    "    '25': ['MA', 'Massachusetts'],\n",
    "    '26': ['MI', 'Michigan'],\n",
    "    '27': ['MN', 'Minnesota'],\n",
    "    '28': ['MS', 'Mississippi'],\n",
    "    '29': ['MO', 'Missouri'],\n",
    "    '30': ['MT', 'Montana'],\n",
    "    '31': ['NE', 'Nebraska'],\n",
    "    '32': ['NV', 'Nevada'],\n",
    "    '33': ['NH', 'New Hampshire'],\n",
    "    '34': ['NJ', 'New Jersey'],\n",
    "    '35': ['NM', 'New Mexico'],\n",
    "    '36': ['NY', 'New York'],\n",
    "    '37': ['NC', 'North Carolina'],\n",
    "    '38': ['ND', 'North Dakota'],\n",
    "    '39': ['OH', 'Ohio'],\n",
    "    '40': ['OK', 'Oklahoma'],\n",
    "    '41': ['OR', 'Oregon'],\n",
    "    '42': ['PA', 'Pennsylvania'],\n",
    "    '44': ['RI', 'Rhode Island'],\n",
    "    '45': ['SC', 'South Carolina'],\n",
    "    '46': ['SD', 'South Dakota'],\n",
    "    '47': ['TN', 'Tennessee'],\n",
    "    '48': ['TX', 'Texas'],\n",
    "    '49': ['UT', 'Utah'],\n",
    "    '50': ['VT', 'Vermont'],\n",
    "    '51': ['VA', 'Virginia'],\n",
    "    '53': ['WA', 'Washington'],\n",
    "    '54': ['WV', 'West Virginia'],\n",
    "    '55': ['WI', 'Wisconsin'],\n",
    "    '56': ['WY', 'Wyoming']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76838e9b-5a85-4e0f-8f9f-500dd7f1b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_folder_pathlib(folder_path):\n",
    "    files_list = []\n",
    "    folder = Path(folder_path)\n",
    "    for file in folder.glob('*.csv'):\n",
    "        if file.is_file():\n",
    "            files_list.append(str(file))\n",
    "        else:\n",
    "            print(f\"{file} is a directory, skipping.\\n\")\n",
    "    return files_list\n",
    "\n",
    "name_to_fips = {info[1]: fips for fips, info in state_fips_to_full_info.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "610d3e23-55ff-4b6e-ad42-8870477f2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_column_names_in_csv(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    csv_files = folder.glob('*.csv')\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if 'subregion2_name' not in df.columns and 'locality_name' in df.columns:\n",
    "            print(f\"Updating '{csv_file.name}': 'locality_name' -> 'subregion2_name'\")\n",
    "            df.rename(columns={'locality_name': 'subregion2_name'}, inplace=True)\n",
    "            df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58126ee1-2cca-4af9-8fa6-91c0955b8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_and_aggregate_data_generic(csv_files, frequency, save_path):\n",
    "    \"\"\"\n",
    "    Process a list of CSV files to aggregate data according to a specified frequency and save the aggregated data to a specified path.\n",
    "    The function also checks for and reports NaN values within each file, and calculates additional statistics such as incidence rates.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_files: List of file paths to CSV files.\n",
    "    - frequency: String specifying the frequency for data aggregation (e.g., 'M' for monthly, 'Y' for yearly).\n",
    "    - save_path: String specifying the path where the aggregated DataFrame should be saved as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing aggregated data for specified columns across all files. This DataFrame is also saved to the specified path.\n",
    "    \"\"\"\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files provided.\")\n",
    "        return None\n",
    "\n",
    "    aggregated_stats = pd.DataFrame()\n",
    "\n",
    "    total_files = len(csv_files)\n",
    "    files_with_nan = 0\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Report the percentage of NaN data in the file\n",
    "        total_values = df.size\n",
    "        nan_values = df.isna().sum().sum()\n",
    "        nan_percentage = (nan_values / total_values) * 100\n",
    "        #print(f\"File '{file_path}' has {nan_percentage:.2f}% NaN values.\")\n",
    "\n",
    "        if nan_percentage > 0:\n",
    "            files_with_nan += 1\n",
    "\n",
    "        required_columns = ['date', 'cumulative_deceased', 'new_deceased', 'subregion2_name', 'population', 'subregion1_name']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            #print(f\"File '{file_path}' is missing columns: {missing_columns}\")\n",
    "            continue  # Skip to the next file\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # Define the aggregation dictionary based on available columns\n",
    "        agg_dict = {\n",
    "            'population': 'last',\n",
    "            'new_deceased': 'sum',\n",
    "            'cumulative_deceased': 'last'\n",
    "        }\n",
    "        if 'new_persons_fully_vaccinated' in df.columns and 'cumulative_persons_fully_vaccinated' in df.columns:\n",
    "            agg_dict.update({\n",
    "                'new_persons_fully_vaccinated': 'sum',\n",
    "                'cumulative_persons_fully_vaccinated': 'last'\n",
    "            })\n",
    "\n",
    "        # Determine grouping columns based on 'subregion' availability\n",
    "        group_cols = ['subregion1_name', 'subregion2_name'] if 'subregion2_name' in df.columns else ['subregion1_name']\n",
    "        group_cols.append(pd.Grouper(key='date', freq=frequency))\n",
    "\n",
    "        # Aggregate data\n",
    "        aggregated_data = df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "        # Map state names to FIPS codes, if 'subregion1_name' is present\n",
    "        if 'subregion1_name' in df.columns:\n",
    "            aggregated_data['state_fips'] = aggregated_data['subregion1_name'].map(name_to_fips)  # Ensure name_to_fips is defined\n",
    "\n",
    "        # Calculate incidence rate and normalize it\n",
    "        if 'new_deceased' in df.columns and 'population' in df.columns:\n",
    "            aggregated_data['incidence_rate'] = (aggregated_data['new_deceased'] / aggregated_data['population']) * 100000\n",
    "            \n",
    "            min_rate = aggregated_data['incidence_rate'].min()\n",
    "            max_rate = aggregated_data['incidence_rate'].max()\n",
    "            if max_rate > min_rate:\n",
    "                aggregated_data['normalized_incidence_rate'] = (aggregated_data['incidence_rate'] - min_rate) / (max_rate - min_rate)\n",
    "            else:\n",
    "                # Handle the case where all values are the same (all zero or constant)\n",
    "                aggregated_data['normalized_incidence_rate'] = 0.0  # or appropriate handling, e.g., np.nan\n",
    "                aggregated_data['color'] = aggregated_data['normalized_incidence_rate'].apply(assign_color)\n",
    "            aggregated_data['color'] = aggregated_data['normalized_incidence_rate'].apply(assign_color)  # Ensure assign_color is defined\n",
    "\n",
    "        # Append to the main DataFrame\n",
    "        aggregated_stats = pd.concat([aggregated_stats, aggregated_data], ignore_index=True)\n",
    "\n",
    "    # Ensure the directory exists before trying to save the file\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the aggregated DataFrame to the specified path\n",
    "    aggregated_stats.to_csv(save_path, index=False)\n",
    "    #print(f\"Aggregated data saved to {save_path}\")\n",
    "    #print(f\"{files_with_nan}/{total_files} files ({(files_with_nan/total_files)*100:.2f}%) had NaN values.\")\n",
    "\n",
    "    return aggregated_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99794c39-027a-4d0f-9b83-7527af1471f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "folder_path = \"../All CSVs\"\n",
    "csv_files = process_files_in_folder_pathlib(folder_path)\n",
    "\n",
    "# Correctly defining frequency list with all values\n",
    "freq_list = ['21D'] #['3D', '7D', '14D', '21D', '28D', 'M', '2M', '3M']\n",
    "\n",
    "for freq in freq_list:\n",
    "    output_path = f'Modified Data/aggregated_monthly_stats_{freq}.csv'\n",
    "    aggregated_data = process_files_and_aggregate_data_generic(csv_files, freq, output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c2106-1b3d-466b-afde-ce636db2c776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
