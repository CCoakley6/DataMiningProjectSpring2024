{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b9b722",
   "metadata": {},
   "source": [
    "# This will be used for binning (Collin believes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0aa612-b368-4498-8477-fea09ff19fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7580cc-3707-4b81-a498-0213b7dd9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "state_fips_to_full_info = {\n",
    "    '01': ['AL', 'Alabama'],\n",
    "    '02': ['AK', 'Alaska'],\n",
    "    '04': ['AZ', 'Arizona'],\n",
    "    '05': ['AR', 'Arkansas'],\n",
    "    '06': ['CA', 'California'],\n",
    "    '08': ['CO', 'Colorado'],\n",
    "    '09': ['CT', 'Connecticut'],\n",
    "    '10': ['DE', 'Delaware'],\n",
    "    '11': ['DC', 'District of Columbia'],\n",
    "    '12': ['FL', 'Florida'],\n",
    "    '13': ['GA', 'Georgia'],\n",
    "    '15': ['HI', 'Hawaii'],\n",
    "    '16': ['ID', 'Idaho'],\n",
    "    '17': ['IL', 'Illinois'],\n",
    "    '18': ['IN', 'Indiana'],\n",
    "    '19': ['IA', 'Iowa'],\n",
    "    '20': ['KS', 'Kansas'],\n",
    "    '21': ['KY', 'Kentucky'],\n",
    "    '22': ['LA', 'Louisiana'],\n",
    "    '23': ['ME', 'Maine'],\n",
    "    '24': ['MD', 'Maryland'],\n",
    "    '25': ['MA', 'Massachusetts'],\n",
    "    '26': ['MI', 'Michigan'],\n",
    "    '27': ['MN', 'Minnesota'],\n",
    "    '28': ['MS', 'Mississippi'],\n",
    "    '29': ['MO', 'Missouri'],\n",
    "    '30': ['MT', 'Montana'],\n",
    "    '31': ['NE', 'Nebraska'],\n",
    "    '32': ['NV', 'Nevada'],\n",
    "    '33': ['NH', 'New Hampshire'],\n",
    "    '34': ['NJ', 'New Jersey'],\n",
    "    '35': ['NM', 'New Mexico'],\n",
    "    '36': ['NY', 'New York'],\n",
    "    '37': ['NC', 'North Carolina'],\n",
    "    '38': ['ND', 'North Dakota'],\n",
    "    '39': ['OH', 'Ohio'],\n",
    "    '40': ['OK', 'Oklahoma'],\n",
    "    '41': ['OR', 'Oregon'],\n",
    "    '42': ['PA', 'Pennsylvania'],\n",
    "    '44': ['RI', 'Rhode Island'],\n",
    "    '45': ['SC', 'South Carolina'],\n",
    "    '46': ['SD', 'South Dakota'],\n",
    "    '47': ['TN', 'Tennessee'],\n",
    "    '48': ['TX', 'Texas'],\n",
    "    '49': ['UT', 'Utah'],\n",
    "    '50': ['VT', 'Vermont'],\n",
    "    '51': ['VA', 'Virginia'],\n",
    "    '53': ['WA', 'Washington'],\n",
    "    '54': ['WV', 'West Virginia'],\n",
    "    '55': ['WI', 'Wisconsin'],\n",
    "    '56': ['WY', 'Wyoming']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a74f8a0-726b-493d-b0b8-948eab1ffaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_column_names_in_csv(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    csv_files = folder.glob('*.csv')\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if 'subregion2_name' not in df.columns and 'locality_name' in df.columns:\n",
    "            print(f\"Updating '{csv_file.name}': 'locality_name' -> 'subregion2_name'\")\n",
    "            df.rename(columns={'locality_name': 'subregion2_name'}, inplace=True)\n",
    "            df.to_csv(csv_file, index=False)\n",
    "\n",
    "def make_color_lighter(color, rate, num_bins):\n",
    "    \"\"\"\n",
    "    Lighten a color based on the bin that a given rate falls into, and return the result as a hexadecimal string.\n",
    "    Each bin corresponds to a certain percentage of lightening.\n",
    "\n",
    "    Args:\n",
    "    color (tuple): The original color as an RGB tuple with values in the range [0, 1].\n",
    "    rate (float): A value between 0 and 1 that determines which bin the color will be categorized into.\n",
    "    num_bins (int): The number of bins, where each bin represents a different lightening intensity.\n",
    "\n",
    "    Returns:\n",
    "    str: The lightened color as a hexadecimal string.\n",
    "    \"\"\"\n",
    "    def rgb_to_hex(rgb):\n",
    "        return '#' + ''.join(f\"{int(round(255 * x)):02x}\" for x in rgb)\n",
    "\n",
    "    white = (1, 1, 1)\n",
    "    # Determine which bin the rate falls into\n",
    "    bin_index = int(rate * num_bins)\n",
    "    # Calculate the lightening percentage for the determined bin\n",
    "    percentage = bin_index / num_bins\n",
    "    # Lighten the color accordingly: more percentage means more white\n",
    "    lightened_color = tuple((1 - percentage) * c + percentage * w for c, w in zip(color, white))\n",
    "    # Convert the lightened RGB color to a hexadecimal string\n",
    "    return rgb_to_hex(lightened_color)   \n",
    "\n",
    "def make_color_lighter(color, rate, bins):\n",
    "\n",
    "    if bins < 1:\n",
    "\n",
    "        raise ValueError(\"Number of bins must be at least 1.\")\n",
    "\n",
    "    if not (0.0 <= rate <= 1.0):\n",
    "\n",
    "        raise ValueError(\"Rate must be between 0.0 and 1.0.\")\n",
    "    # Define start and end colors in RGB\n",
    "\n",
    "    start_color = (0, 255, 0)  # green\n",
    "\n",
    "    end_color = (255, 0, 0)    # Red\n",
    "\n",
    "    # Calculate the step change for each component\n",
    "    step = [(end_color[i] - start_color[i]) / (bins - 1) for i in range(3)]\n",
    "\n",
    "    # Calculate the color index based on the rate\n",
    "    color_index = int(rate * (bins - 1))\n",
    "\n",
    "    # Calculate the final color for the given rate\n",
    "    final_color = [int(start_color[i] + step[i] * color_index) for i in range(3)]\n",
    "    # Convert RGB to Hexadecimal\n",
    "\n",
    "    return '#{:02X}{:02X}{:02X}'.format(*final_color)\n",
    "            \n",
    "def assign_color(rate):\n",
    "    if pd.isna(rate):\n",
    "        return '#808080'\n",
    "    else:\n",
    "        red = int(rate * 255)\n",
    "        green = 255 - red\n",
    "        blue = 0\n",
    "    return '#{:02x}{:02x}{:02x}'.format(red, green, blue)\n",
    "\n",
    "def process_files_in_folder_pathlib(folder_path):\n",
    "    files_list = []\n",
    "    folder = Path(folder_path)\n",
    "    for file in folder.glob('*.csv'):\n",
    "        if file.is_file():\n",
    "            files_list.append(str(file))\n",
    "        else:\n",
    "            print(f\"{file} is a directory, skipping.\\n\")\n",
    "    return files_list\n",
    "\n",
    "name_to_fips = {info[1]: fips for fips, info in state_fips_to_full_info.items()}\n",
    "\n",
    "\n",
    "\n",
    "def process_files_and_aggregate_data_generic(csv_files, frequency, save_path):\n",
    "    \"\"\"\n",
    "    Process a list of CSV files to aggregate data according to a specified frequency and save the aggregated data to a specified path.\n",
    "    The function also checks for and reports NaN values within each file, and calculates additional statistics such as incidence rates.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_files: List of file paths to CSV files.\n",
    "    - frequency: String specifying the frequency for data aggregation (e.g., 'M' for monthly, 'Y' for yearly).\n",
    "    - save_path: String specifying the path where the aggregated DataFrame should be saved as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing aggregated data for specified columns across all files. This DataFrame is also saved to the specified path.\n",
    "    \"\"\"\n",
    "    color = (0.5, 0, 0) #dark read\n",
    "    num_bins = 5 #number of bins\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files provided.\")\n",
    "        return None\n",
    "\n",
    "    aggregated_stats = pd.DataFrame()\n",
    "\n",
    "    total_files = len(csv_files)\n",
    "    files_with_nan = 0\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Report the percentage of NaN data in the file\n",
    "        total_values = df.size\n",
    "        nan_values = df.isna().sum().sum()\n",
    "        nan_percentage = (nan_values / total_values) * 100\n",
    "        #print(f\"File '{file_path}' has {nan_percentage:.2f}% NaN values.\")\n",
    "\n",
    "        if nan_percentage > 0:\n",
    "            files_with_nan += 1\n",
    "\n",
    "        required_columns = ['date', 'cumulative_deceased', 'new_deceased', 'subregion2_name', 'population', 'subregion1_name']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            #print(f\"File '{file_path}' is missing columns: {missing_columns}\")\n",
    "            continue  # Skip to the next file\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # Define the aggregation dictionary based on available columns\n",
    "        agg_dict = {\n",
    "            'population': 'last',\n",
    "            'new_deceased': 'sum',\n",
    "            'cumulative_deceased': 'last'\n",
    "        }\n",
    "        if 'new_persons_fully_vaccinated' in df.columns and 'cumulative_persons_fully_vaccinated' in df.columns:\n",
    "            agg_dict.update({\n",
    "                'new_persons_fully_vaccinated': 'sum',\n",
    "                'cumulative_persons_fully_vaccinated': 'last'\n",
    "            })\n",
    "\n",
    "        # Determine grouping columns based on 'subregion' availability\n",
    "        group_cols = ['subregion1_name', 'subregion2_name'] if 'subregion2_name' in df.columns else ['subregion1_name']\n",
    "        group_cols.append(pd.Grouper(key='date', freq=frequency))\n",
    "\n",
    "        # Aggregate data\n",
    "        aggregated_data = df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "        # Map state names to FIPS codes, if 'subregion1_name' is present\n",
    "        if 'subregion1_name' in df.columns:\n",
    "            aggregated_data['state_fips'] = aggregated_data['subregion1_name'].map(name_to_fips)  # Ensure name_to_fips is defined\n",
    "\n",
    "        # Calculate incidence rate and normalize it\n",
    "        if 'new_deceased' in df.columns and 'population' in df.columns:\n",
    "            aggregated_data.loc[aggregated_data['new_deceased'] < 0, 'new_deceased'] = 0\n",
    "            aggregated_data['incidence_rate'] = (aggregated_data['new_deceased'] / aggregated_data['population']) * 100000\n",
    "            \n",
    "            min_rate = aggregated_data['incidence_rate'].min()\n",
    "            max_rate = aggregated_data['incidence_rate'].max()\n",
    "            if max_rate > min_rate:\n",
    "                aggregated_data['normalized_incidence_rate'] = (aggregated_data['incidence_rate'] - min_rate) / (max_rate - min_rate)\n",
    "            else:\n",
    "                # Handle the case where all values are the same (all zero or constant)\n",
    "                aggregated_data['normalized_incidence_rate'] = 0.0 # or appropriate handling, e.g., np.nan\n",
    "            aggregated_data['color'] = aggregated_data['normalized_incidence_rate'].apply(lambda rate: make_color_lighter(color, rate, num_bins))# Ensure assign_color is defined\n",
    "\n",
    "        # Append to the main DataFrame\n",
    "        aggregated_stats = pd.concat([aggregated_stats, aggregated_data], ignore_index=True)\n",
    "\n",
    "    # Ensure the directory exists before trying to save the file\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the aggregated DataFrame to the specified path\n",
    "    aggregated_stats.to_csv(save_path, index=False)\n",
    "    #print(f\"Aggregated data saved to {save_path}\")\n",
    "    #print(f\"{files_with_nan}/{total_files} files ({(files_with_nan/total_files)*100:.2f}%) had NaN values.\")\n",
    "\n",
    "    return aggregated_stats\n",
    "\n",
    "folder_path = \"../All CSVs\"\n",
    "csv_files = process_files_in_folder_pathlib(folder_path)\n",
    "\n",
    "# Correctly defining frequency list with all values\n",
    "# Correctly defining frequency list with all values\n",
    "freq_list = ['6M','1Y']\n",
    "\n",
    "for freq in freq_list:\n",
    "    output_path = f'Modified Data/aggregated_stats_4BinsMethod{freq}.csv'\n",
    "    aggregated_data = process_files_and_aggregate_data_generic(csv_files, freq, output_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cbafbf",
   "metadata": {},
   "source": [
    "# This is the start of our 4/18/2024 changes to Gil's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4ccdb0-e79a-42e1-a844-331dca9fbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac64b4ba-1bac-43b5-8e92-0ff2e9b7865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_color(rate):\n",
    "    if pd.isna(rate):\n",
    "        return '#808080'\n",
    "    else:\n",
    "        red = int(rate * 255)\n",
    "        green = 255 - red\n",
    "        blue = 0\n",
    "        return '#{:02x}{:02x}{:02x}'.format(red, green, blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dadc229e-51e9-4fe5-a2f6-78ffb1ecdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_fips_to_full_info = {\n",
    "    '01': ['AL', 'Alabama'],\n",
    "    '02': ['AK', 'Alaska'],\n",
    "    '04': ['AZ', 'Arizona'],\n",
    "    '05': ['AR', 'Arkansas'],\n",
    "    '06': ['CA', 'California'],\n",
    "    '08': ['CO', 'Colorado'],\n",
    "    '09': ['CT', 'Connecticut'],\n",
    "    '10': ['DE', 'Delaware'],\n",
    "    '11': ['DC', 'District of Columbia'],\n",
    "    '12': ['FL', 'Florida'],\n",
    "    '13': ['GA', 'Georgia'],\n",
    "    '15': ['HI', 'Hawaii'],\n",
    "    '16': ['ID', 'Idaho'],\n",
    "    '17': ['IL', 'Illinois'],\n",
    "    '18': ['IN', 'Indiana'],\n",
    "    '19': ['IA', 'Iowa'],\n",
    "    '20': ['KS', 'Kansas'],\n",
    "    '21': ['KY', 'Kentucky'],\n",
    "    '22': ['LA', 'Louisiana'],\n",
    "    '23': ['ME', 'Maine'],\n",
    "    '24': ['MD', 'Maryland'],\n",
    "    '25': ['MA', 'Massachusetts'],\n",
    "    '26': ['MI', 'Michigan'],\n",
    "    '27': ['MN', 'Minnesota'],\n",
    "    '28': ['MS', 'Mississippi'],\n",
    "    '29': ['MO', 'Missouri'],\n",
    "    '30': ['MT', 'Montana'],\n",
    "    '31': ['NE', 'Nebraska'],\n",
    "    '32': ['NV', 'Nevada'],\n",
    "    '33': ['NH', 'New Hampshire'],\n",
    "    '34': ['NJ', 'New Jersey'],\n",
    "    '35': ['NM', 'New Mexico'],\n",
    "    '36': ['NY', 'New York'],\n",
    "    '37': ['NC', 'North Carolina'],\n",
    "    '38': ['ND', 'North Dakota'],\n",
    "    '39': ['OH', 'Ohio'],\n",
    "    '40': ['OK', 'Oklahoma'],\n",
    "    '41': ['OR', 'Oregon'],\n",
    "    '42': ['PA', 'Pennsylvania'],\n",
    "    '44': ['RI', 'Rhode Island'],\n",
    "    '45': ['SC', 'South Carolina'],\n",
    "    '46': ['SD', 'South Dakota'],\n",
    "    '47': ['TN', 'Tennessee'],\n",
    "    '48': ['TX', 'Texas'],\n",
    "    '49': ['UT', 'Utah'],\n",
    "    '50': ['VT', 'Vermont'],\n",
    "    '51': ['VA', 'Virginia'],\n",
    "    '53': ['WA', 'Washington'],\n",
    "    '54': ['WV', 'West Virginia'],\n",
    "    '55': ['WI', 'Wisconsin'],\n",
    "    '56': ['WY', 'Wyoming']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76838e9b-5a85-4e0f-8f9f-500dd7f1b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_in_folder_pathlib(folder_path):\n",
    "    files_list = []\n",
    "    folder = Path(folder_path)\n",
    "    for file in folder.glob('*.csv'):\n",
    "        if file.is_file():\n",
    "            files_list.append(str(file))\n",
    "        else:\n",
    "            print(f\"{file} is a directory, skipping.\\n\")\n",
    "    return files_list\n",
    "\n",
    "name_to_fips = {info[1]: fips for fips, info in state_fips_to_full_info.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610d3e23-55ff-4b6e-ad42-8870477f2656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_column_names_in_csv(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    csv_files = folder.glob('*.csv')\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        if 'subregion2_name' not in df.columns and 'locality_name' in df.columns:\n",
    "            print(f\"Updating '{csv_file.name}': 'locality_name' -> 'subregion2_name'\")\n",
    "            df.rename(columns={'locality_name': 'subregion2_name'}, inplace=True)\n",
    "            df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58126ee1-2cca-4af9-8fa6-91c0955b8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files_and_aggregate_data_generic(csv_files, frequency, save_path):\n",
    "    \"\"\"\n",
    "    Process a list of CSV files to aggregate data according to a specified frequency and save the aggregated data to a specified path.\n",
    "    The function also checks for and reports NaN values within each file, and calculates additional statistics such as incidence rates.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_files: List of file paths to CSV files.\n",
    "    - frequency: String specifying the frequency for data aggregation (e.g., 'M' for monthly, 'Y' for yearly).\n",
    "    - save_path: String specifying the path where the aggregated DataFrame should be saved as a CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing aggregated data for specified columns across all files. This DataFrame is also saved to the specified path.\n",
    "    \"\"\"\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files provided.\")\n",
    "        return None\n",
    "\n",
    "    aggregated_stats = pd.DataFrame()\n",
    "\n",
    "    total_files = len(csv_files)\n",
    "    files_with_nan = 0\n",
    "\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Report the percentage of NaN data in the file\n",
    "        total_values = df.size\n",
    "        nan_values = df.isna().sum().sum()\n",
    "        nan_percentage = (nan_values / total_values) * 100\n",
    "        #print(f\"File '{file_path}' has {nan_percentage:.2f}% NaN values.\")\n",
    "\n",
    "        if nan_percentage > 0:\n",
    "            files_with_nan += 1\n",
    "\n",
    "        required_columns = ['date', 'cumulative_deceased', 'new_deceased', 'subregion2_name', 'population', 'subregion1_name']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_columns:\n",
    "            #print(f\"File '{file_path}' is missing columns: {missing_columns}\")\n",
    "            continue  # Skip to the next file\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # Define the aggregation dictionary based on available columns\n",
    "        agg_dict = {\n",
    "            'population': 'last',\n",
    "            'new_deceased': 'sum',\n",
    "            'cumulative_deceased': 'last'\n",
    "        }\n",
    "        if 'new_persons_fully_vaccinated' in df.columns and 'cumulative_persons_fully_vaccinated' in df.columns:\n",
    "            agg_dict.update({\n",
    "                'new_persons_fully_vaccinated': 'sum',\n",
    "                'cumulative_persons_fully_vaccinated': 'last'\n",
    "            })\n",
    "\n",
    "        # Determine grouping columns based on 'subregion' availability\n",
    "        group_cols = ['subregion1_name', 'subregion2_name'] if 'subregion2_name' in df.columns else ['subregion1_name']\n",
    "        group_cols.append(pd.Grouper(key='date', freq=frequency))\n",
    "\n",
    "        # Aggregate data\n",
    "        aggregated_data = df.groupby(group_cols).agg(agg_dict).reset_index()\n",
    "\n",
    "        # Map state names to FIPS codes, if 'subregion1_name' is present\n",
    "        if 'subregion1_name' in df.columns:\n",
    "            aggregated_data['state_fips'] = aggregated_data['subregion1_name'].map(name_to_fips)  # Ensure name_to_fips is defined\n",
    "\n",
    "        # Calculate incidence rate and normalize it\n",
    "        if 'new_deceased' in df.columns and 'population' in df.columns:\n",
    "            aggregated_data.loc[aggregated_data['new_deceased'] < 0, 'new_deceased'] = 0\n",
    "            aggregated_data['incidence_rate'] = (aggregated_data['new_deceased'] / aggregated_data['population']) * 100000\n",
    "            \n",
    "            min_rate = aggregated_data['incidence_rate'].min()\n",
    "            max_rate = aggregated_data['incidence_rate'].max()\n",
    "            if max_rate > min_rate:\n",
    "                aggregated_data['normalized_incidence_rate'] = (aggregated_data['incidence_rate'] - min_rate) / (max_rate - min_rate)\n",
    "            else:\n",
    "                # Handle the case where all values are the same (all zero or constant)\n",
    "                aggregated_data['normalized_incidence_rate'] = 0.0 # or appropriate handling, e.g., np.nan\n",
    "            aggregated_data['color'] = aggregated_data['normalized_incidence_rate'].apply(assign_color)  # Ensure assign_color is defined\n",
    "\n",
    "        # Append to the main DataFrame\n",
    "        aggregated_stats = pd.concat([aggregated_stats, aggregated_data], ignore_index=True)\n",
    "\n",
    "    # Ensure the directory exists before trying to save the file\n",
    "    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the aggregated DataFrame to the specified path\n",
    "    aggregated_stats.to_csv(save_path, index=False)\n",
    "    #print(f\"Aggregated data saved to {save_path}\")\n",
    "    #print(f\"{files_with_nan}/{total_files} files ({(files_with_nan/total_files)*100:.2f}%) had NaN values.\")\n",
    "\n",
    "    return aggregated_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99794c39-027a-4d0f-9b83-7527af1471f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "folder_path = \"../All CSVs\"\n",
    "csv_files = process_files_in_folder_pathlib(folder_path)\n",
    "\n",
    "# Correctly defining frequency list with all values\n",
    "freq_list = [ '6M', '1Y']\n",
    "\n",
    "for freq in freq_list:\n",
    "    output_path = f'Modified Data/aggregated_monthly_stats_{freq}.csv'\n",
    "    aggregated_data = process_files_and_aggregate_data_generic(csv_files, freq, output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c2106-1b3d-466b-afde-ce636db2c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def plot_counties_from_csv(shapefile_gdf, csv_file_path, output_folder, title_format=\"Plot for {}\"):\n",
    "    # Define the continental US bounds\n",
    "    continental_us_bounds = {\n",
    "        \"minx\": -130,\n",
    "        \"miny\": 24,\n",
    "        \"maxx\": -66,\n",
    "        \"maxy\": 50\n",
    "    }\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Rename columns for merging\n",
    "    df.rename(columns={'state_fips': 'STATEFP', 'subregion2_name': 'NAMELSAD'}, inplace=True)\n",
    "\n",
    "    # Convert the 'date' column to datetime for easier manipulation\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # Group the DataFrame by 'date'\n",
    "    grouped = df.groupby('date')\n",
    "\n",
    "    # Make sure the output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for date, group in grouped:\n",
    "        # Create a color dictionary from the group\n",
    "        color_dict = {row['NAMELSAD']: row['color'] for idx, row in group.iterrows()}\n",
    "        \n",
    "        # Update the 'color' column in the shapefile GeoDataFrame based on the color dictionary\n",
    "        shapefile_gdf['color'] = shapefile_gdf['NAMELSAD'].map(color_dict).fillna('#808080')\n",
    "\n",
    "        # Plotting setup\n",
    "        fig, ax = plt.subplots(figsize=(15, 8), dpi=300)\n",
    "        ax.set_xlim(continental_us_bounds[\"minx\"], continental_us_bounds[\"maxx\"])\n",
    "        ax.set_ylim(continental_us_bounds[\"miny\"], continental_us_bounds[\"maxy\"])\n",
    "\n",
    "        # Plot the counties with a neutral color to provide a base map\n",
    "        shapefile_gdf.plot(ax=ax, color='lightgrey', edgecolor='black', linewidth=0.4)\n",
    "        \n",
    "        # Plot the counties with colors from the 'color' column\n",
    "        shapefile_gdf.plot(ax=ax, color=shapefile_gdf['color'], edgecolor='black', linewidth=0.4)\n",
    "\n",
    "        # Title and saving\n",
    "        title_date = date.strftime('%Y-%m-%d')\n",
    "        ax.set_title(title_format.format(title_date))\n",
    "        ax.set_axis_off()\n",
    "        output_png_path = os.path.join(output_folder, f\"{title_date}.png\")\n",
    "        plt.savefig(output_png_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "# File paths\n",
    "csv_file_path = r\"C:\\Users\\A404007\\Desktop\\Gils Folder\\Data Mining\\projectdata\\ChangedData\\aggregated_stats_4BinsMethod14D.csv\"\n",
    "shp_file_path = r'C:\\Users\\A404007\\Desktop\\Gils Folder\\Data Mining\\projectdata\\GeoShapeData\\tl_2023_us_county\\tl_2023_us_county.shp'\n",
    "output_folder = r'C:\\Users\\A404007\\Desktop\\Gils Folder\\Data Mining\\projectdata\\GeoPlot'\n",
    "\n",
    "# Load shapefile into GeoDataFrame\n",
    "shape_df = gpd.read_file(shp_file_path)\n",
    "#shape_df.sort_values(by=['STATEFP', 'NAMELSAD'], ascending=[True, True], inplace=True) #SORTED\n",
    "# Call the function\n",
    "title_format = \"COVID-19 Incidence on {}\"\n",
    "plot_counties_from_csv(shape_df, csv_file_path, output_folder, title_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a47db5d-899f-4a95-b88b-0b27fdd21431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gils code for plots. Example use below. This plots a scaling too. \n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib.colorbar import ColorbarBase\n",
    "\n",
    "def plot_counties_from_csv(shapefile_gdf, csv_file_path, output_folder, title_format=\"Plot for {}\"):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    df.rename(columns={'state_fips': 'STATEFP', 'subregion2_name': 'NAMELSAD'}, inplace=True)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    grouped = df.groupby('date')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    colors = [\"#00FF00\", \"#3FBF00\", \"#7F7F00\", \"#BF3F00\", \"#FF0000\"]\n",
    "    cmap = LinearSegmentedColormap.from_list(\"rate_scale\", colors, N=5)\n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "\n",
    "    for date, group in grouped:\n",
    "        color_dict = {row['NAMELSAD']: row['color'] for idx, row in group.iterrows()}\n",
    "        shapefile_gdf['color'] = shapefile_gdf['NAMELSAD'].map(color_dict).fillna('#808080')\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(15, 10))  # Increased height to accommodate the colorbar below\n",
    "        ax.set_xlim([-130, -66])\n",
    "        ax.set_ylim([24, 50])\n",
    "        shapefile_gdf.plot(ax=ax, color='lightgrey', edgecolor='black', linewidth=0.4)\n",
    "        shapefile_gdf.plot(ax=ax, color=shapefile_gdf['color'], edgecolor='black', linewidth=0.4)\n",
    "        \n",
    "        # Adjust the main plot's position to leave space for the colorbar\n",
    "        ax_position = ax.get_position()  # Get the bounding box of the main plot\n",
    "        ax.set_position([ax_position.x0, ax_position.y0 + ax_position.height * 0.1,  # Adjust bottom\n",
    "                         ax_position.width, ax_position.height * 0.9])  # Adjust height\n",
    "\n",
    "        # Create a new axes for the colorbar at the bottom\n",
    "        colorbar_ax = fig.add_axes([ax_position.x0, ax_position.y0 * 0.1, ax_position.width, 0.03])  # Position for the colorbar\n",
    "        \n",
    "        # Create and configure the colorbar\n",
    "        cbar = ColorbarBase(colorbar_ax, cmap=cmap, norm=norm, orientation='horizontal')\n",
    "        cbar.set_label('Rate')\n",
    "        cbar.set_ticks([0.12, 0.37, 0.62, 0.825, 1.0])\n",
    "        cbar.set_ticklabels(['0.0-0.24', '0.25-0.49', '0.5-0.74', '0.75-0.90', '1.0'])\n",
    "        \n",
    "        # Set the title and save the figure\n",
    "        ax.set_title(title_format.format(date.strftime('%Y-%m-%d')))\n",
    "        ax.set_axis_off()\n",
    "        plt.savefig(os.path.join(output_folder, f\"{date.strftime('%Y-%m-%d')}.png\"), dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "# File paths\n",
    "csv_file_path = r\"C:\\Users\\A404007\\Desktop\\Gils Folder\\Data Mining\\projectdata\\ChangedData\\aggregated_stats_4BinsMethod14D.csv\"\n",
    "shp_file_path = r'C:\\Users\\A404007\\Desktop\\Gils Folder\\Data Mining\\projectdata\\GeoShapeData\\tl_2023_us_county\\tl_2023_us_county.shp'\n",
    "output_folder = r'C:\\Users\\A404007\\Desktop\\Gils Folder\\Data Mining\\projectdata\\GeoPlot'\n",
    "\n",
    "# Load shapefile into GeoDataFrame\n",
    "shape_df = gpd.read_file(shp_file_path)\n",
    "#shape_df.sort_values(by=['STATEFP', 'NAMELSAD'], ascending=[True, True], inplace=True) #SORTED\n",
    "# Call the function\n",
    "title_format = \"COVID-19 Incidence on {}\"\n",
    "plot_counties_from_csv(shape_df, csv_file_path, output_folder, title_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7525563f-6c55-4b56-9ca8-ada74ce43a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
